{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/moviedata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>character_name</th>\n",
       "      <th>line_num</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avengers</td>\n",
       "      <td>Nick Fury</td>\n",
       "      <td>0</td>\n",
       "      <td>how bad is it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avengers</td>\n",
       "      <td>Nick Fury</td>\n",
       "      <td>1</td>\n",
       "      <td>nasa didn't authorize selvig to test phase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avengers</td>\n",
       "      <td>Nick Fury</td>\n",
       "      <td>2</td>\n",
       "      <td>what are the energy levels now?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avengers</td>\n",
       "      <td>Nick Fury</td>\n",
       "      <td>3</td>\n",
       "      <td>how long to get everyone out?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avengers</td>\n",
       "      <td>Nick Fury</td>\n",
       "      <td>4</td>\n",
       "      <td>do better.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movie character_name  line_num  \\\n",
       "0  Avengers      Nick Fury         0   \n",
       "1  Avengers      Nick Fury         1   \n",
       "2  Avengers      Nick Fury         2   \n",
       "3  Avengers      Nick Fury         3   \n",
       "4  Avengers      Nick Fury         4   \n",
       "\n",
       "                                          line  \n",
       "0                               how bad is it?  \n",
       "1  nasa didn't authorize selvig to test phase.  \n",
       "2              what are the energy levels now?  \n",
       "3                how long to get everyone out?  \n",
       "4                                   do better.  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Data\n",
    "To get the data ready for machine learning, we need to tokenize, filter out one-word occurrences, and vectorize the tokens.\n",
    "\n",
    "## Tokenizing \n",
    "Our first step is to tokenize. We will include unigrams *and* bigrams in our set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.dropna().copy(deep=True)\n",
    "\n",
    "unigrams = data2['line'].copy(deep=True).apply(word_tokenize)\n",
    "bigrams = data2['line'].copy(deep=True\n",
    "                    ).apply(word_tokenize\n",
    "                    ).apply(bigrams\n",
    "                    ).apply(list\n",
    "                    ).apply(lambda x: ['_'.join(bigram) for bigram in x])\n",
    "\n",
    "data2['tokens'] = unigrams + bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'bad', 'is', 'it', '?', 'how_bad', 'bad_is', 'is_it', 'it_?']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2['tokens'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing 1-Count Occurrences\n",
    "Before including the tokens in the final data set, we will need to filter out 1-count occurrences from the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [how, bad, is, it, ?, how_bad, bad_is, is_it, ...\n",
       "1    [did, n't, selvig, to, test, phase, ., nasa_di...\n",
       "2    [what, are, the, energy, levels, now, ?, what_...\n",
       "3    [how, long, to, get, everyone, out, ?, how_lon...\n",
       "4                 [do, better, ., do_better, better_.]\n",
       "Name: tokens_filtered, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The resulting from the code below is found in \n",
    "# data/moviedata_tokens.csv, so you don't have \n",
    "# to run the code below. However, if you want to \n",
    "# for any reason, uncomment and run!\n",
    "\n",
    "'''\n",
    "# flatten the list of unigram tokens into a single list of words\n",
    "words = [word for token_list in unigrams for word in token_list]\n",
    "\n",
    "# create frequency distribution of the words\n",
    "freq_dist = nltk.FreqDist(words)\n",
    "\n",
    "# Filter out words with a count of 1\n",
    "uni_filtered_words = [word for word in words if freq_dist[word] > 1]\n",
    "\n",
    "# Combine filtered unigrams with bigrams\n",
    "bi_words = [word for token_list in bigrams for word in token_list]\n",
    "filtered_words = uni_filtered_words + bi_words\n",
    "\n",
    "# Remove 1-count occurrences from the tokenized text column\n",
    "data2['tokens_filtered'] = data2['tokens'].apply(\n",
    "    lambda x: [word for word in x if word in filtered_words])\n",
    "\n",
    "data2['tokens_filtered'].head()\n",
    "\n",
    "data3 = data2.drop(columns='tokens').rename(\n",
    "    columns = {'tokens_filtered': 'tokens'})\n",
    "\n",
    "data3.to_csv('data/moviedata_tokens.csv', index=False)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you ran the code above and need to\n",
    "# re-export to csv. (not recommended)\n",
    "\n",
    "# data3.to_csv('data/moviedata_tokens.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
