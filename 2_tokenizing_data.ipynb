{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Data\n",
    "To get the data ready for machine learning, we need to tokenize and filter out one-word occurrences before we can vectorize the data and fit a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/moviedata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>character_name</th>\n",
       "      <th>line_num</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American Psycho</td>\n",
       "      <td>Bateman</td>\n",
       "      <td>0</td>\n",
       "      <td>we're sitting in pastels, this nouvelle northe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Psycho</td>\n",
       "      <td>Bateman</td>\n",
       "      <td>1</td>\n",
       "      <td>you'll notice that my friends and i all look a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>American Psycho</td>\n",
       "      <td>Bateman</td>\n",
       "      <td>2</td>\n",
       "      <td>or can it be worn with a suit?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American Psycho</td>\n",
       "      <td>Bateman</td>\n",
       "      <td>3</td>\n",
       "      <td>with discreet pinstripes you should wear a sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Psycho</td>\n",
       "      <td>Bateman</td>\n",
       "      <td>4</td>\n",
       "      <td>van patten looks puffy. has he stopped working...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             movie character_name  line_num  \\\n",
       "0  American Psycho        Bateman         0   \n",
       "1  American Psycho        Bateman         1   \n",
       "2  American Psycho        Bateman         2   \n",
       "3  American Psycho        Bateman         3   \n",
       "4  American Psycho        Bateman         4   \n",
       "\n",
       "                                                line  \n",
       "0  we're sitting in pastels, this nouvelle northe...  \n",
       "1  you'll notice that my friends and i all look a...  \n",
       "2                     or can it be worn with a suit?  \n",
       "3  with discreet pinstripes you should wear a sub...  \n",
       "4  van patten looks puffy. has he stopped working...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing \n",
    "Our first step is to tokenize. We will include unigrams *and* bigrams in our set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.dropna().copy(deep=True)\n",
    "\n",
    "unigrams = data2['line'].copy(deep=True).apply(word_tokenize)\n",
    "bigrams = data2['line'].copy(deep=True\n",
    "                    ).apply(word_tokenize\n",
    "                    ).apply(bigrams\n",
    "                    ).apply(list\n",
    "                    ).apply(lambda x: ['_'.join(bigram) for bigram in x])\n",
    "\n",
    "data2['tokens'] = unigrams + bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['or',\n",
       " 'can',\n",
       " 'it',\n",
       " 'be',\n",
       " 'worn',\n",
       " 'with',\n",
       " 'a',\n",
       " 'suit',\n",
       " '?',\n",
       " 'or_can',\n",
       " 'can_it',\n",
       " 'it_be',\n",
       " 'be_worn',\n",
       " 'worn_with',\n",
       " 'with_a',\n",
       " 'a_suit',\n",
       " 'suit_?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2['tokens'][2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing 1-Count Occurrences\n",
    "Before including the tokens in the final data set, we will need to filter out 1-count occurrences from the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting from the code below is found in \n",
    "# data/moviedata_tokens.csv, so you don't have \n",
    "# to run the code below. However, if you want to \n",
    "# for any reason, uncomment and run!\n",
    "'''\n",
    "# flatten the list of unigram tokens into a single list of words\n",
    "words = [word for token_list in unigrams for word in token_list]\n",
    "\n",
    "# create frequency distribution of the words\n",
    "freq_dist = nltk.FreqDist(words)\n",
    "\n",
    "# Filter out words with a count of 1\n",
    "uni_filtered_words = [word for word in words if freq_dist[word] > 1]\n",
    "\n",
    "# Create unigrams column\n",
    "data2['unigram_tokens'] = data2['tokens'].apply(\n",
    "    lambda x: [word for word in x if word in uni_filtered_words])\n",
    ")\n",
    "\n",
    "# Combine filtered unigrams with bigrams\n",
    "bi_words = [word for token_list in bigrams for word in token_list]\n",
    "filtered_words = uni_filtered_words + bi_words\n",
    "\n",
    "# Remove 1-count occurrences from the tokenized text column\n",
    "data2['tokens_filtered'] = data2['tokens'].apply(\n",
    "    lambda x: [word for word in x if word in filtered_words])\n",
    "\n",
    "data2['tokens_filtered'].head()\n",
    "\n",
    "data3 = data2.drop(columns='tokens').rename(\n",
    "    columns = {'tokens_filtered': 'bigram_unigram_tokens'})\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you ran the code above and need to\n",
    "# re-export to csv. (not recommended)\n",
    "\n",
    "#data3.to_csv('data/moviedata_tokens.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "Now that we've got our tokenized data, we can move onto vectorizing and fitting our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
